{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "* In essence, refactoring code is the process of making small changes to the structure of the code while maintaining its public behavior.\n",
    "* The main purpose of refactoring is to increase the efficiency in the various parts of the project.\n",
    "    * Programmer's efficiency - Reducing code complexity, improving readability.\n",
    "    * System efficiency - Optimizing performance of the code, reducing the potential for bugs.\n",
    "* For instance, take the sample Python code snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log 1: foo\n",
      "\n",
      "log 2: bar\n",
      "\n",
      "log 3: baz\n",
      "\n",
      "log 4: qux\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple operation - Writing a piece of information to a log file\n",
    "import os # Cleaning up created test file\n",
    "\n",
    "# Setup sample data\n",
    "logs = [\"log 1: foo\", \"log 2: bar\", \"log 3: baz\", \"log 4: qux\"]\n",
    "\n",
    "# Appending the data\n",
    "appended_logs: str = \"\"\n",
    "for log in logs:\n",
    "    appended_logs += f\"{log}\\n\"\n",
    "    \n",
    "# Writing the data\n",
    "opened_file = open(\"testlog.txt\", \"w\")\n",
    "opened_file.write(appended_logs)\n",
    "opened_file.close()\n",
    "\n",
    "# Validating the data has been written into a file\n",
    "reopened_file = open(f\"testlog.txt\", \"r\")\n",
    "for line in reopened_file.readlines():\n",
    "    print(line)\n",
    "reopened_file.close()\n",
    "\n",
    "# Performing cleanup\n",
    "os.remove(\"testlog.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The example above appears to be working as expected with the correct output being created from the read file - This works as a MVP.\n",
    "* However, there are several opportunities for refactoring:\n",
    "    * The programmer has to remember to call the close() method of the file object each time after the file is written to/ read from.\n",
    "        * This creates an additional mental burden on the programmer.\n",
    "        * Failure to close files may lead to dangling file handles, eventually resulting in OSErrors and/or possibility of corrupted files.\n",
    "        * Purpose of refactoring: **Reducing the potential for bugs.**\n",
    "    * Current method of creating strings is relatively inefficient and may cause future scalability issues.\n",
    "        * Due to the immutable nature of strings in Python, appending to an existing string requires allocation of a new string instance, with the existing string being copied over in memory.\n",
    "        * With a small number of log messages there is virtually no measurable impact. However, as the number of log messages increases, this will become a growing problem.\n",
    "        * Purpose of refactoring: **Increasing performance.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log 1: foo\n",
      "\n",
      "log 2: bar\n",
      "\n",
      "log 3: baz\n",
      "\n",
      "log 4: qux\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Refactored version\n",
    "\n",
    "import os # Cleaning up created test file\n",
    "import io\n",
    "\n",
    "# Setup sample data\n",
    "logs = [\"log 1: foo\", \"log 2: bar\", \"log 3: baz\", \"log 4: qux\"]\n",
    "\n",
    "# Appending the data\n",
    "appended_logs_buffer = io.StringIO() # Writing to a mutable buffer.\n",
    "for log in logs:\n",
    "    appended_logs_buffer.write(f\"{log}\\n\") # Buffer will reallocate memory instead of performing copy\n",
    "appended_logs = appended_logs_buffer.getvalue()\n",
    "\n",
    "# Writing the data\n",
    "# Using context managers will obviate the need to call the close() method.\n",
    "# This make the code less error prone.\n",
    "with open(\"testlog.txt\", \"w\") as opened_file:\n",
    "    opened_file.write(appended_logs)\n",
    "    \n",
    "# Validating the data has been written into a file\n",
    "with open(\"testlog.txt\", \"r\") as reopened_file:\n",
    "    for line in reopened_file:\n",
    "        print(line)\n",
    "\n",
    "# Performing cleanup\n",
    "os.remove(\"testlog.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To summarize, refactoring the code by re-writing certain portions of it has made it more robust and performant.\n",
    "* However, in reality performance optimization are usually only be carried out after the hot code paths are identified.\n",
    "    * The example above is lacking in this regard since it does performance optimization without any benchmarks.\n",
    "    * Perhaps the use case is only for tens of log messages, in which case using a mutable buffer over the naive method does not yield any noticeable performance increases since other overheads may dominate.\n",
    "* One import aspect to also consider during refactoring is that shorter code blocks may not always be the better option.\n",
    "    * Concise code is usually preferred, but sometimes longer code blocks that lays out the explicit steps may be preferable to shorter blocks that abstract away the important details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing\n",
    "* Each time we write a block of code, we expect the code to behave in a certain way.\n",
    "    * This expected behavior is then usually documented in the form of documentations, docstrings etc.\n",
    "* For future users/maintainers of our code, the expectation is that the code behaves exactly according to the documentation.\n",
    "* However, there may be certain edge case which the original developer did not consider.\n",
    "    * These edge cases can then cause the code to behave unexpectedly.\n",
    "* Hence, the main objective of unit tests is to test various edge conditions against a relatively small logical code block to ensure that it conforms to the expected behavior as per documentation.\n",
    "* Unit Testing may also be part of a software development strategy, such as Test-Driven Development.\n",
    "\n",
    "## When to employ unit tests\n",
    "* Unit tests should be employed whenever a logical block of code is written with the intent for production use.\n",
    "    * Logical blocks in this case implies a block of functional code - It can be a function, method, class etc.\n",
    "\n",
    "## Writing Test Cases\n",
    "* Test cases should be written to test for behavior not implementation.\n",
    "* This means that the focus of testing should be on the public API of the function/method/class, not on how it is implemented.\n",
    "* If tests were designed around the latter, then any optimizations efforts would likely cause unit tests to fail even though the external behaviour was identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example class\n",
    "class Summation:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.total_count = 0\n",
    "    \n",
    "    def summed(self, n: int) -> int:\n",
    "        \"\"\"\n",
    "        Summation of all values from 1 to n\n",
    "        \"\"\"\n",
    "        self.total_count = 0\n",
    "        \n",
    "        i = 1\n",
    "        while i <= n:\n",
    "            self.total_count += i\n",
    "            i += 1\n",
    "        result = self.total_count\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above shows an trivial example of a summation class.\n",
    "* Below shows the 2 ways we can write tests for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing implementation\n",
    "\n",
    "# Arrange\n",
    "test1 = Summation()\n",
    "\n",
    "# Act\n",
    "test1.summed(5)\n",
    "\n",
    "# Assert\n",
    "assert test1.total_count == 15 # Reliant on the instance property\n",
    "\n",
    "# Testing behavior\n",
    "\n",
    "# Arrange\n",
    "test1 = Summation()\n",
    "\n",
    "# Act\n",
    "result = test1.summed(5)\n",
    "\n",
    "# Assert\n",
    "assert result == 15\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As seen above, both sets of tests pass.\n",
    "* However, if we may eventually refactor the class to optimize the peformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example class\n",
    "class Summation:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        return None # No longer has an internal counter.\n",
    "    \n",
    "    def summed(self, n: int) -> int:\n",
    "        \"\"\"\n",
    "        Summation of all values from 1 to n\n",
    "        \"\"\"\n",
    "        return int(0.5*n*(n+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior test passes!\n",
      "Implementation test fails!\n"
     ]
    }
   ],
   "source": [
    "# Testing behavior\n",
    "\n",
    "# Arrange\n",
    "test1 = Summation()\n",
    "\n",
    "# Act\n",
    "result = test1.summed(5)\n",
    "\n",
    "# Assert\n",
    "assert result == 15\n",
    "\n",
    "print(f\"Behavior test passes!\")\n",
    "\n",
    "# Testing implementation\n",
    "\n",
    "# Arrange\n",
    "test1 = Summation()\n",
    "\n",
    "# Act\n",
    "test1.summed(5)\n",
    "\n",
    "# Assert\n",
    "try:\n",
    "    assert test1.total_count == 15 # Reliant on the instance property\n",
    "except:\n",
    "    print(\"Implementation test fails!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Running the unit tests after refactoring shows that the tests that tested the implementation had failed, while the behavioral tests passes.\n",
    "* This again serves to illustrate that when writing test cases, it is undesirable to have failing test when the behavior is the same, and the solution to mitigate this occurence is via writing behavioral tests instead of implementation tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
